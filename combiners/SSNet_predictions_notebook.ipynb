{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2128fff-6647-439e-aeed-3d6036759c90",
   "metadata": {},
   "source": [
    "# SSNet Predictions\n",
    "\n",
    "This notebook is meant for hands-on interaction with the code and data used in `SSNet_predictions.py`. Annotations explaining the general functioning of each section and the other modules they reference are provided. Similar notebooks may be added for individual models and combiners in the future. Note that the code shown here does not necessarily reflect the content of the script version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63568fc5-2757-421a-bbd8-4cb471db3faf",
   "metadata": {},
   "source": [
    "This cell can be run to easily convert this notebook to a Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e138dd-bc45-42d4-868f-0d37e2a37f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook SSNet_predictions_notebook.ipynb to script\n",
      "[NbConvertApp] Writing 17807 bytes to SSNet_predictions_notebook.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script SSNet_predictions_notebook.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70050e31-7d4b-4f80-82f1-0ce7d54b72bc",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "365f9f09-7fa7-4a5d-982f-92c3a77f3e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis is master file that runs all the three combiners proposed in the paper. \\nUse following snippet to run all the three combiners: python SSNet_predictions.py\\nPlease note that this code has tensorflow dependencies.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "==================================================LICENSING TERMS==================================================\n",
    "This code and data was developed by employees of the National Institute of Standards and Technology (NIST), an agency of the Federal Government. Pursuant to title 17 United States Code Section 105, works of NIST employees are not subject to copyright protection in the United States and are considered to be in the public domain. The code and data is provided by NIST as a public service and is expressly provided \"AS IS.\" NIST MAKES NO WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR STATUTORY, INCLUDING, WITHOUT LIMITATION, THE IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT AND DATA ACCURACY. NIST does not warrant or make any representations regarding the use of the data or the results thereof, including but not limited to the correctness, accuracy, reliability or usefulness of the data. NIST SHALL NOT BE LIABLE AND YOU HEREBY RELEASE NIST FROM LIABILITY FOR ANY INDIRECT, CONSEQUENTIAL, SPECIAL, OR INCIDENTAL DAMAGES (INCLUDING DAMAGES FOR LOSS OF BUSINESS PROFITS, BUSINESS INTERRUPTION, LOSS OF BUSINESS INFORMATION, AND THE LIKE), WHETHER ARISING IN TORT, CONTRACT, OR OTHERWISE, ARISING FROM OR RELATING TO THE DATA (OR THE USE OF OR INABILITY TO USE THIS DATA), EVEN IF NIST HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n",
    "To the extent that NIST may hold copyright in countries other than the United States, you are hereby granted the non-exclusive irrevocable and unconditional right to print, publish, prepare derivative works and distribute the NIST data, in any medium, or authorize others to do so on your behalf, on a royalty-free basis throughout the world.\n",
    "You may improve, modify, and create derivative works of the code or the data or any portion of the code or the data, and you may copy and distribute such modifications or works. Modified works should carry a notice stating that you changed the code or the data and should note the date and nature of any such change. Please explicitly acknowledge the National Institute of Standards and Technology as the source of the code or the data: Citation recommendations are provided below. Permission to use this code and data is contingent upon your acceptance of the terms of this agreement and upon your providing appropriate acknowledgments of NIST's creation of the code and data.\n",
    "Paper Title:\n",
    "    SSNet: a Sagittal Stratum-inspired Neural Network Framework for Sentiment Analysis\n",
    "SSNet authors and developers:\n",
    "    Apostol Vassilev:\n",
    "        Affiliation: National Institute of Standards and Technology\n",
    "        Email: apostol.vassilev@nist.gov\n",
    "    Munawar Hasan:\n",
    "        Affiliation: National Institute of Standards and Technology\n",
    "        Email: munawar.hasan@nist.gov\n",
    "    Jin Honglan\n",
    "        Affiliation: National Institute of Standards and Technology\n",
    "        Email: honglan.jin@nist.gov\n",
    "====================================================================================================================\n",
    "'''\n",
    "\n",
    "'''\n",
    "This is master file that runs all the three combiners proposed in the paper. \n",
    "Use following snippet to run all the three combiners: python SSNet_predictions.py\n",
    "Please note that this code has tensorflow dependencies.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92fd944-e689-4daa-a782-915db10f206e",
   "metadata": {},
   "source": [
    "## Imports/Dependencies\n",
    "\n",
    "TensorFlow is the main machine learning framework used to implement, train, and apply the models. Pandas and NumPy are used for general data preprocessing and manipulation. Components from Matplotlib/Pyplot and IPython which are absent from `SSNet_predictions.py` are utilized here to provide enhanced interactivity and visualization. Functions from the following scripts (corresponding to the combiner models described in the paper) are imported:\n",
    " - `SSNet_Neural_Network.py`\n",
    " - `SSNet_Bayesian_Decision.py`\n",
    " - `SSNet_Heuristic_Hybrid.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ae03c1f-dd3f-472d-bfcd-1b4c389fe4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import JSON\n",
    "import itertools\n",
    "\n",
    "from SSNet_Neural_Network import nn\n",
    "from SSNet_Bayesian_Decision import bayesian_decision\n",
    "from SSNet_Heuristic_Hybrid import heuristic_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f451afe-7bd7-4850-80d9-20e722514919",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_5ktr = 'imdb_train_5k.csv'\n",
    "model_a_tr = 'model_1_5ktrain.csv'\n",
    "model_b_tr = 'model_2_5ktrain.csv'\n",
    "# model_c_tr = 'model_3_bert_result_train_5k.csv'\n",
    "# model_d_tr = 'model_4_use_result_train_5k.csv'\n",
    "model_c_tr = 'model_3_5ktrain.csv'\n",
    "model_d_tr = 'model_4_5ktrain.csv'\n",
    "\n",
    "model_a_te = 'model_1_25ktest.csv'\n",
    "model_b_te = 'model_2_25ktest.csv'\n",
    "# model_c_te = 'model_3_bert_result_test_25k.csv'\n",
    "# model_d_te = 'model_4_use_result_test_25k.csv'\n",
    "model_c_te = 'model_3_25ktest.csv'\n",
    "model_d_te = 'model_4_25ktest.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67dc15d-759a-44c6-996c-1e7552d3ba81",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1effdb21-842a-4141-ad5a-3ae4f9e5777a",
   "metadata": {},
   "source": [
    "### Training Dict Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e5e7a31-3838-4e40-be2c-1836d48fd708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dict_threshold(split):\n",
    "    training_dict = dict()\n",
    "\n",
    "    if split == \"5K\":\n",
    "        training_dict[\"5K\"] = [\n",
    "            [model_a_tr, model_b_tr, model_c_tr, model_d_tr], [\n",
    "                model_a_te, model_b_te, model_c_te, model_d_te]\n",
    "        ]\n",
    "        \n",
    "    return training_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbd0ec35-8af4-45ab-9b4b-faf4a40fde83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', '2'), ('1', '3'), ('1', '4'), ('2', '3'), ('2', '4'), ('3', '4')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.combinations('1234', 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284fb2fc-2920-406e-9c66-d3bffdc2baf0",
   "metadata": {},
   "source": [
    "### Training Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d4c530b-93cd-4717-83cd-bcc81a712bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "model_{1,2,3,4}": [
        [
         "model_1_5ktrain.csv",
         "model_2_5ktrain.csv",
         "model_3_5ktrain.csv",
         "model_4_5ktrain.csv"
        ],
        [
         "model_1_25ktest.csv",
         "model_2_25ktest.csv",
         "model_3_25ktest.csv",
         "model_4_25ktest.csv"
        ]
       ],
       "model_{1,2,3}": [
        [
         "model_1_5ktrain.csv",
         "model_2_5ktrain.csv",
         "model_3_5ktrain.csv"
        ],
        [
         "model_1_25ktest.csv",
         "model_2_25ktest.csv",
         "model_3_25ktest.csv"
        ]
       ],
       "model_{1,2,4}": [
        [
         "model_1_5ktrain.csv",
         "model_2_5ktrain.csv",
         "model_4_5ktrain.csv"
        ],
        [
         "model_1_25ktest.csv",
         "model_2_25ktest.csv",
         "model_4_25ktest.csv"
        ]
       ],
       "model_{1,2}": [
        [
         "model_1_5ktrain.csv",
         "model_2_5ktrain.csv"
        ],
        [
         "model_1_25ktest.csv",
         "model_2_25ktest.csv"
        ]
       ],
       "model_{1,3,4}": [
        [
         "model_1_5ktrain.csv",
         "model_3_5ktrain.csv",
         "model_4_5ktrain.csv"
        ],
        [
         "model_1_25ktest.csv",
         "model_3_25ktest.csv",
         "model_4_25ktest.csv"
        ]
       ],
       "model_{1,3}": [
        [
         "model_1_5ktrain.csv",
         "model_3_5ktrain.csv"
        ],
        [
         "model_1_25ktest.csv",
         "model_3_25ktest.csv"
        ]
       ],
       "model_{1,4}": [
        [
         "model_1_5ktrain.csv",
         "model_4_5ktrain.csv"
        ],
        [
         "model_1_25ktest.csv",
         "model_4_25ktest.csv"
        ]
       ],
       "model_{2,3,4}": [
        [
         "model_2_5ktrain.csv",
         "model_3_5ktrain.csv",
         "model_4_5ktrain.csv"
        ],
        [
         "model_2_25ktest.csv",
         "model_3_25ktest.csv",
         "model_4_25ktest.csv"
        ]
       ],
       "model_{2,3}": [
        [
         "model_2_5ktrain.csv",
         "model_3_5ktrain.csv"
        ],
        [
         "model_2_25ktest.csv",
         "model_3_25ktest.csv"
        ]
       ],
       "model_{2,4}": [
        [
         "model_2_5ktrain.csv",
         "model_4_5ktrain.csv"
        ],
        [
         "model_2_25ktest.csv",
         "model_4_25ktest.csv"
        ]
       ],
       "model_{3,4}": [
        [
         "model_3_5ktrain.csv",
         "model_4_5ktrain.csv"
        ],
        [
         "model_3_25ktest.csv",
         "model_4_25ktest.csv"
        ]
       ]
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_training_dict(split):\n",
    "    training_dict = dict()\n",
    "#     Store a running list of model combinations\n",
    "#     included_models = []\n",
    "    if split == \"5K\":\n",
    "#         Loop through integers 2 to 4 (inclusive); the number of component models in each combination\n",
    "        for r in range(2, 5):\n",
    "#             Generate combinations of model indices (without repetition)\n",
    "            for c in itertools.combinations(map(str, range(1, 5)), r):\n",
    "                training_dict['model_{{{}}}'.format(','.join(c))] = [\n",
    "#                     Generate the file names corresponding to each model's output on both the training and testing data\n",
    "                    [f'model_{m}_{n}{t}.csv' for m in c] for n, t in [\n",
    "                        ('5k', 'train'),\n",
    "                        ('25k', 'test')\n",
    "                    ]\n",
    "                ]\n",
    "\n",
    "    return training_dict\n",
    "\n",
    "# Test the function\n",
    "JSON(get_training_dict(\"5K\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e7f04a-645d-4344-8712-e2146481938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile a list of text files containing reviews and their corresponding sentiment labels\n",
    "\n",
    "imdb_25k_list = list()\n",
    "data_dir = 'models/train'\n",
    "for file_name in os.listdir(f'../{data_dir}/pos'):\n",
    "    if file_name != '.DS_Store':\n",
    "        imdb_25k_list.append([file_name, str(1)])\n",
    "\n",
    "for file_name in os.listdir(f'../{data_dir}/neg'):\n",
    "    if file_name != '.DS_Store':\n",
    "        imdb_25k_list.append([file_name, str(0)])\n",
    "\n",
    "SAMPLE_SPLIT = [\"5K\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6645716b-095e-4808-852b-1ba069a5891a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imdb_25k_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d4cbe7d-9ebb-4a4a-a619-7376d1c4d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789679a-0be1-4a61-8351-9cfefd494764",
   "metadata": {},
   "source": [
    "## Train Predictors\n",
    "\n",
    "Train the predictors and return the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a72be80b-83e6-4c09-b353-c7c44f75bd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predictor():\n",
    "    for split in SAMPLE_SPLIT:\n",
    "        print(\"Sample Split: \", split)\n",
    "        imdb_list = list()\n",
    "        training_dict = None\n",
    "        training_dict_threshold = None\n",
    "\n",
    "        if split == \"5K\":\n",
    "            df_imdb_tr = pd.read_csv(imdb_5ktr)\n",
    "            for index in df_imdb_tr.index:\n",
    "                file_name = str(df_imdb_tr['file'][index])\n",
    "                label = int(df_imdb_tr['label'][index])\n",
    "                imdb_list.append([file_name, str(label)])\n",
    "\n",
    "            random.shuffle(imdb_list)\n",
    "            training_dict = get_training_dict(split)\n",
    "            training_dict_threshold = get_training_dict_threshold(split)\n",
    "\n",
    "            random.shuffle(imdb_list)\n",
    "            training_dict = get_training_dict(split)\n",
    "            training_dict_threshold = get_training_dict_threshold(split)\n",
    "\n",
    "\n",
    "        acc_dict_nn = dict()\n",
    "        acc_dict_bdc = dict()\n",
    "\n",
    "        for k, v in training_dict.items():\n",
    "\n",
    "            tr_list = list()\n",
    "            te_list = list()\n",
    "\n",
    "            for i in range(len(v[0])):\n",
    "                df = pd.read_csv(v[0][i])\n",
    "                df_dict = dict()\n",
    "\n",
    "                for idx in df.index:\n",
    "                    file_name = str(df['file'][idx])\n",
    "                    proba = float(df['prob'][idx])\n",
    "                    df_dict[file_name] = proba\n",
    "\n",
    "                tr_list.append(df_dict)\n",
    "\n",
    "            for i in range(len(v[1])):\n",
    "                df = pd.read_csv(v[1][i])\n",
    "                df_dict = dict()\n",
    "\n",
    "                for idx in df.index:\n",
    "                    file_name = str(df['file'][idx])\n",
    "                    proba = float(df['prob'][idx])\n",
    "                    df_dict[file_name] = proba\n",
    "\n",
    "                te_list.append(df_dict)\n",
    "\n",
    "            assert len(tr_list) == len(te_list), \"train and test samples mismatch ....\"\n",
    "            tr_acc = -1.\n",
    "            te_acc = -1.\n",
    "            while True:\n",
    "                tr_acc, te_acc, weights = nn(tr_list=tr_list, imdb_tr_list=imdb_list,\n",
    "                    te_list=te_list, imdb_te_list=imdb_25k_list)\n",
    "                model_weights.append(weights)\n",
    "\n",
    "                if weights[0][0] == 0. or weights[0][1] == 0.:\n",
    "                    print(\"bad event ...., training again\")\n",
    "                    print(\"\\t\" +k)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            acc_dict_nn[k] = [tr_acc, te_acc]\n",
    "\n",
    "            acc_dict_bdc[k] = bayesian_decision(tr_list=tr_list, imdb_tr_list=imdb_list,\n",
    "                                             te_list=te_list, imdb_te_list=imdb_25k_list)\n",
    "\n",
    "\n",
    "        for k, v in training_dict_threshold.items():\n",
    "            tr_list = list()\n",
    "            te_list = list()\n",
    "\n",
    "            for i in range(len(v[0])):\n",
    "                df = pd.read_csv(v[0][i])\n",
    "                df_dict = dict()\n",
    "\n",
    "                for idx in df.index:\n",
    "                    file_name = str(df['file'][idx])\n",
    "                    proba = float(df['prob'][idx])\n",
    "                    df_dict[file_name] = proba\n",
    "\n",
    "                tr_list.append(df_dict)\n",
    "\n",
    "            for i in range(len(v[1])):\n",
    "                df = pd.read_csv(v[1][i])\n",
    "                df_dict = dict()\n",
    "\n",
    "                for idx in df.index:\n",
    "                    file_name = str(df['file'][idx])\n",
    "                    proba = float(df['prob'][idx])\n",
    "                    df_dict[file_name] = proba\n",
    "\n",
    "                te_list.append(df_dict)\n",
    "\n",
    "        hh_dict = heuristic_hybrid(tr_list=tr_list, imdb_tr_list=imdb_list,\n",
    "                        te_list=te_list, imdb_te_list=imdb_25k_list)\n",
    "\n",
    "        nn_metrics = []\n",
    "        \n",
    "#         Print summaries of the results for each combiner\n",
    "        #print(\"Training Complete: \")\n",
    "        print(\"Neural Network Combiner: \")\n",
    "        for k, v in acc_dict_nn.items():\n",
    "            print(\"\\t\" +k +\": training accuracy = \" +str(v[0]) + \", test accuracy = \" +str(v[1]))\n",
    "            nn_metrics.append(v)\n",
    "            \n",
    "        bdr_metrics = []\n",
    "        print(\"\\n\")\n",
    "        print(\"Bayesian Decision Rule Combiner: \")\n",
    "        for k, v in acc_dict_bdc.items():\n",
    "            print(\"\\t\" +k)\n",
    "            for i, j in v.items():\n",
    "                print(\"\\t\\t\" +i +\": training accuracy = \" +str(j[0]) +\", test accuracy = \" +str(j[1]))\n",
    "            bdr_metrics.append(v)\n",
    "        \n",
    "        hh_metrics = []\n",
    "        print(\"\\n\")\n",
    "        print(\"Heuristic-Hybrid Combiner: \")\n",
    "        for k, v in hh_dict.items():\n",
    "            print(\"Base:\", k)\n",
    "            for index in range(len(v)):\n",
    "                print(\"\\t\\t\", v[index])\n",
    "            print(\"\\n\")\n",
    "            hh_metrics.append(v)\n",
    "            \n",
    "        return nn_metrics, bdr_metrics, hh_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "419f403a-8547-4620-8fa5-9137218b1dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.9200919],\n",
      "       [0.8884677]], dtype=float32), array([[0.6129163],\n",
      "       [1.011195 ]], dtype=float32), array([[-0.],\n",
      "       [-0.]], dtype=float32), array([[0.9654697],\n",
      "       [0.7502592]], dtype=float32), array([[0.6691013],\n",
      "       [1.1757464]], dtype=float32), array([[1.0892915 ],\n",
      "       [0.91290605]], dtype=float32), array([[1.1838671 ],\n",
      "       [0.59295523]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "W = [m[0] for m in model_weights[:7]]\n",
    "print(W)\n",
    "# plt.pcolormesh(model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1918602a-a859-4ce2-a561-22a23da37641",
   "metadata": {},
   "source": [
    "## Result Aggregation\n",
    "\n",
    "Runs the predictor training script and displays the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fa7ac9d-9cd8-4539-9eef-ca4754d1e09a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Split:  5K\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "bad event ...., training again\n",
      "\tmodel_{2,3}\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "bad event ...., training again\n",
      "\tmodel_{2,4}\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "bad event ...., training again\n",
      "\tmodel_{1,2,4}\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "bad event ...., training again\n",
      "\tmodel_{1,2,4}\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "bad event ...., training again\n",
      "\tmodel_{1,3,4}\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "bad event ...., training again\n",
      "\tmodel_{2,3,4}\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "bad event ...., training again\n",
      "\tmodel_{2,3,4}\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "bad event ...., training again\n",
      "\tmodel_{2,3,4}\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "bad event ...., training again\n",
      "\tmodel_{1,2,3,4}\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "25000 25000\n",
      "Neural Network Combiner: \n",
      "\tmodel_{1,2}: training accuracy = 90.82000255584717, test accuracy = 90.58800000000001\n",
      "\tmodel_{1,3}: training accuracy = 93.32000017166138, test accuracy = 93.568\n",
      "\tmodel_{1,4}: training accuracy = 90.02000093460083, test accuracy = 89.888\n",
      "\tmodel_{2,3}: training accuracy = 93.48000288009644, test accuracy = 93.64\n",
      "\tmodel_{2,4}: training accuracy = 91.11999869346619, test accuracy = 91.03999999999999\n",
      "\tmodel_{3,4}: training accuracy = 93.27999949455261, test accuracy = 93.58800000000001\n",
      "\tmodel_{1,2,3}: training accuracy = 93.80000233650208, test accuracy = 93.884\n",
      "\tmodel_{1,2,4}: training accuracy = 91.28000140190125, test accuracy = 91.244\n",
      "\tmodel_{1,3,4}: training accuracy = 93.66000294685364, test accuracy = 93.756\n",
      "\tmodel_{2,3,4}: training accuracy = 93.33999752998352, test accuracy = 93.71199999999999\n",
      "\tmodel_{1,2,3,4}: training accuracy = 93.77999901771545, test accuracy = 93.65599999999999\n",
      "\n",
      "\n",
      "Bayesian Decision Rule Combiner: \n",
      "\tmodel_{1,2}\n",
      "\t\tmax: training accuracy = 90.75999999999999, test accuracy = 90.324\n",
      "\t\tavg: training accuracy = 90.75999999999999, test accuracy = 90.324\n",
      "\t\tsum: training accuracy = 90.75999999999999, test accuracy = 90.324\n",
      "\tmodel_{1,3}\n",
      "\t\tmax: training accuracy = 93.7, test accuracy = 93.512\n",
      "\t\tavg: training accuracy = 93.7, test accuracy = 93.512\n",
      "\t\tsum: training accuracy = 93.7, test accuracy = 93.512\n",
      "\tmodel_{1,4}\n",
      "\t\tmax: training accuracy = 90.24, test accuracy = 89.812\n",
      "\t\tavg: training accuracy = 90.24, test accuracy = 89.812\n",
      "\t\tsum: training accuracy = 90.24, test accuracy = 89.812\n",
      "\tmodel_{2,3}\n",
      "\t\tmax: training accuracy = 93.04, test accuracy = 93.508\n",
      "\t\tavg: training accuracy = 93.04, test accuracy = 93.508\n",
      "\t\tsum: training accuracy = 93.04, test accuracy = 93.508\n",
      "\tmodel_{2,4}\n",
      "\t\tmax: training accuracy = 90.92, test accuracy = 91.10000000000001\n",
      "\t\tavg: training accuracy = 90.92, test accuracy = 91.10000000000001\n",
      "\t\tsum: training accuracy = 90.92, test accuracy = 91.10000000000001\n",
      "\tmodel_{3,4}\n",
      "\t\tmax: training accuracy = 92.66, test accuracy = 92.92399999999999\n",
      "\t\tavg: training accuracy = 92.66, test accuracy = 92.92399999999999\n",
      "\t\tsum: training accuracy = 92.66, test accuracy = 92.92399999999999\n",
      "\tmodel_{1,2,3}\n",
      "\t\tmax: training accuracy = 93.36, test accuracy = 93.616\n",
      "\t\tavg: training accuracy = 92.64, test accuracy = 92.864\n",
      "\t\tsum: training accuracy = 92.64, test accuracy = 92.864\n",
      "\t\tmaj: training accuracy = 92.2, test accuracy = 92.144\n",
      "\tmodel_{1,2,4}\n",
      "\t\tmax: training accuracy = 91.3, test accuracy = 91.18\n",
      "\t\tavg: training accuracy = 90.86, test accuracy = 91.06400000000001\n",
      "\t\tsum: training accuracy = 90.86, test accuracy = 91.06400000000001\n",
      "\t\tmaj: training accuracy = 90.60000000000001, test accuracy = 90.668\n",
      "\tmodel_{1,3,4}\n",
      "\t\tmax: training accuracy = 93.17999999999999, test accuracy = 93.144\n",
      "\t\tavg: training accuracy = 92.2, test accuracy = 92.252\n",
      "\t\tsum: training accuracy = 92.2, test accuracy = 92.252\n",
      "\t\tmaj: training accuracy = 91.84, test accuracy = 91.724\n",
      "\tmodel_{2,3,4}\n",
      "\t\tmax: training accuracy = 92.82000000000001, test accuracy = 93.33200000000001\n",
      "\t\tavg: training accuracy = 92.86, test accuracy = 92.896\n",
      "\t\tsum: training accuracy = 92.86, test accuracy = 92.896\n",
      "\t\tmaj: training accuracy = 92.47999999999999, test accuracy = 92.528\n",
      "\tmodel_{1,2,3,4}\n",
      "\t\tmax: training accuracy = 93.02, test accuracy = 93.364\n",
      "\t\tavg: training accuracy = 92.80000000000001, test accuracy = 92.84400000000001\n",
      "\t\tsum: training accuracy = 92.80000000000001, test accuracy = 92.84400000000001\n",
      "\n",
      "\n",
      "Heuristic-Hybrid Combiner: \n",
      "Base: model_{1}\n",
      "\t\t {'tr_acc': 90.44, 'te_acc': 90.064, 'th': 0.92}\n",
      "\t\t {'tr_acc': 93.08, 'te_acc': 92.624, 'th': 0.95}\n",
      "\t\t {'tr_acc': 89.8, 'te_acc': 89.44800000000001, 'th': 0.89}\n",
      "\t\t {'tr_acc': 93.18, 'te_acc': 93.27600000000001, 'th': 0.97}\n",
      "\t\t {'tr_acc': 93.18, 'te_acc': 93.27600000000001, 'th': 0.97}\n",
      "\t\t {'tr_acc': 93.18, 'te_acc': 93.27600000000001, 'th': 0.97}\n",
      "\t\t {'tr_acc': 91.14, 'te_acc': 91.072, 'th': 0.95}\n",
      "\t\t {'tr_acc': 91.14, 'te_acc': 91.072, 'th': 0.95}\n",
      "\t\t {'tr_acc': 91.14, 'te_acc': 91.072, 'th': 0.95}\n",
      "\t\t {'tr_acc': 92.84, 'te_acc': 92.784, 'th': 0.97}\n",
      "\t\t {'tr_acc': 92.84, 'te_acc': 92.784, 'th': 0.97}\n",
      "\t\t {'tr_acc': 92.84, 'te_acc': 92.784, 'th': 0.97}\n",
      "\t\t {'tr_acc': 92.86, 'te_acc': 93.28, 'th': 0.99}\n",
      "\t\t {'tr_acc': 92.88, 'te_acc': 92.88, 'th': 0.99}\n",
      "\t\t {'tr_acc': 92.88, 'te_acc': 92.88, 'th': 0.99}\n",
      "\t\t {'tr_acc': 92.62, 'te_acc': 92.524, 'th': 0.99}\n",
      "\n",
      "\n",
      "Base: model_{2}\n",
      "\t\t {'tr_acc': 90.28, 'te_acc': 90.076, 'th': 0.82}\n",
      "\t\t {'tr_acc': 92.72, 'te_acc': 92.708, 'th': 0.97}\n",
      "\t\t {'tr_acc': 90.9, 'te_acc': 90.672, 'th': 0.84}\n",
      "\t\t {'tr_acc': 93.42, 'te_acc': 93.408, 'th': 0.99}\n",
      "\t\t {'tr_acc': 93.42, 'te_acc': 93.408, 'th': 0.99}\n",
      "\t\t {'tr_acc': 93.42, 'te_acc': 93.408, 'th': 0.99}\n",
      "\t\t {'tr_acc': 91.14, 'te_acc': 90.924, 'th': 0.95}\n",
      "\t\t {'tr_acc': 91.14, 'te_acc': 90.924, 'th': 0.95}\n",
      "\t\t {'tr_acc': 91.14, 'te_acc': 90.924, 'th': 0.95}\n",
      "\t\t {'tr_acc': 92.7, 'te_acc': 92.892, 'th': 0.98}\n",
      "\t\t {'tr_acc': 92.7, 'te_acc': 92.892, 'th': 0.98}\n",
      "\t\t {'tr_acc': 92.7, 'te_acc': 92.892, 'th': 0.98}\n",
      "\t\t {'tr_acc': 93.02, 'te_acc': 93.144, 'th': 0.99}\n",
      "\t\t {'tr_acc': 92.32, 'te_acc': 92.376, 'th': 0.97}\n",
      "\t\t {'tr_acc': 92.32, 'te_acc': 92.376, 'th': 0.97}\n",
      "\t\t {'tr_acc': 92.06, 'te_acc': 91.988, 'th': 0.97}\n",
      "\n",
      "\n",
      "Base: model_{3}\n",
      "\t\t {'tr_acc': 93.36, 'te_acc': 93.416, 'th': 0.83}\n",
      "\t\t {'tr_acc': 93.38, 'te_acc': 93.352, 'th': 0.83}\n",
      "\t\t {'tr_acc': 93.0, 'te_acc': 93.116, 'th': 0.75}\n",
      "\t\t {'tr_acc': 93.48, 'te_acc': 93.596, 'th': 0.88}\n",
      "\t\t {'tr_acc': 93.48, 'te_acc': 93.596, 'th': 0.88}\n",
      "\t\t {'tr_acc': 93.48, 'te_acc': 93.596, 'th': 0.88}\n",
      "\t\t {'tr_acc': 93.32, 'te_acc': 93.27600000000001, 'th': 0.75}\n",
      "\t\t {'tr_acc': 93.32, 'te_acc': 93.27600000000001, 'th': 0.75}\n",
      "\t\t {'tr_acc': 93.32, 'te_acc': 93.27600000000001, 'th': 0.75}\n",
      "\t\t {'tr_acc': 93.26, 'te_acc': 93.296, 'th': 0.75}\n",
      "\t\t {'tr_acc': 93.26, 'te_acc': 93.296, 'th': 0.75}\n",
      "\t\t {'tr_acc': 93.26, 'te_acc': 93.296, 'th': 0.75}\n",
      "\t\t {'tr_acc': 93.36, 'te_acc': 93.34, 'th': 0.75}\n",
      "\t\t {'tr_acc': 93.4, 'te_acc': 93.428, 'th': 0.76}\n",
      "\t\t {'tr_acc': 93.4, 'te_acc': 93.428, 'th': 0.76}\n",
      "\t\t {'tr_acc': 93.4, 'te_acc': 93.41199999999999, 'th': 0.76}\n",
      "\n",
      "\n",
      "Base: model_{4}\n",
      "\t\t {'tr_acc': 89.78, 'te_acc': 89.676, 'th': 0.94}\n",
      "\t\t {'tr_acc': 90.66, 'te_acc': 90.436, 'th': 0.93}\n",
      "\t\t {'tr_acc': 92.74, 'te_acc': 92.572, 'th': 0.99}\n",
      "\t\t {'tr_acc': 91.08, 'te_acc': 90.80799999999999, 'th': 0.99}\n",
      "\t\t {'tr_acc': 91.08, 'te_acc': 90.80799999999999, 'th': 0.99}\n",
      "\t\t {'tr_acc': 91.08, 'te_acc': 90.80799999999999, 'th': 0.99}\n",
      "\t\t {'tr_acc': 93.42, 'te_acc': 93.07600000000001, 'th': 0.99}\n",
      "\t\t {'tr_acc': 93.42, 'te_acc': 93.07600000000001, 'th': 0.99}\n",
      "\t\t {'tr_acc': 93.42, 'te_acc': 93.07600000000001, 'th': 0.99}\n",
      "\t\t {'tr_acc': 92.96, 'te_acc': 93.05600000000001, 'th': 0.99}\n",
      "\t\t {'tr_acc': 92.96, 'te_acc': 93.05600000000001, 'th': 0.99}\n",
      "\t\t {'tr_acc': 92.96, 'te_acc': 93.05600000000001, 'th': 0.99}\n",
      "\t\t {'tr_acc': 93.12, 'te_acc': 93.128, 'th': 0.99}\n",
      "\t\t {'tr_acc': 92.56, 'te_acc': 92.56400000000001, 'th': 0.99}\n",
      "\t\t {'tr_acc': 92.56, 'te_acc': 92.56400000000001, 'th': 0.99}\n",
      "\t\t {'tr_acc': 92.2, 'te_acc': 92.032, 'th': 0.99}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trials = []\n",
    "for i in range(1):\n",
    "    results = train_predictor()\n",
    "    trials.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7866bd82-1468-49cf-b3bf-49f24d4d9bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list(d):\n",
    "    L = [v for k, v in d.items()]\n",
    "    return L\n",
    "\n",
    "# Convert result dictionaries to lists\n",
    "for k, t in enumerate(trials):\n",
    "    for i in range(len(trials[k])):\n",
    "        for j, m in enumerate(trials[k][i]):\n",
    "        #     for k, v in m.items():\n",
    "            if type(m) is dict:\n",
    "                trials[k][i][j] = to_list(m)\n",
    "            for j2, m2 in enumerate(m):\n",
    "                if type(m2) is dict:\n",
    "                    trials[k][i][j][j2] = to_list(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5397d6fe-5876-4f94-92f4-b479bf4f2aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(list(trials))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d742fcd4-1aad-4460-938b-8917b8cda2f4",
   "metadata": {},
   "source": [
    "## Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb24cdfd-3a9c-4d4b-b6b3-01e8c067420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_scienceplots = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b4c49d-224e-4bbb-a5be-fc2eaa4f0700",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = trials[0][0]\n",
    "# print(trials[0][0])\n",
    "\n",
    "l = ['Train', 'Test']\n",
    "combiner_names = [\n",
    "    'Neural Network',\n",
    "    'Bayesian Decision Rule',\n",
    "    'Heuristic-Hybrid'\n",
    "]\n",
    "bdr_props = ['Max', 'Avg', 'Sum', 'Majority']\n",
    "hh_props = l + ['Threshold']\n",
    "label_data = [l, bdr_props, hh_props]\n",
    "\n",
    "# plot_style = 'ggplot'\n",
    "if use_scienceplots:\n",
    "    try:\n",
    "        plt.style.use('science')\n",
    "        plt.style.use(['science','no-latex'])\n",
    "    except:\n",
    "        pass\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def grouped_plot(C=0, sections=2, reduce=0):\n",
    "    labels = [x.split('_')[-1] for x in get_training_dict('5K').keys()]\n",
    "    values = trials[0][C]\n",
    "#     print(values)\n",
    "#     valshape = np.array(values).shape\n",
    "#     if len(valshape) == 3:\n",
    "#         values\n",
    "\n",
    "    reduced = False\n",
    "    combiner_type = combiner_names[C]\n",
    "    if 'Bayesian' in combiner_type:\n",
    "        for i, v in enumerate(values):\n",
    "            for j, vi in enumerate(v):\n",
    "                if type(vi) in [list, tuple]:\n",
    "                    values[i][j] = vi[reduce]\n",
    "    elif 'Heuristic' in combiner_type:\n",
    "#         print(values)\n",
    "        values = values[reduce]\n",
    "#         print(values)\n",
    "        \n",
    "    if any(g in combiner_type for g in ['Bayesian', 'Heuristic']):\n",
    "        reduced = True\n",
    "    # plt.bar(labels, trials[0][0])\n",
    "\n",
    "    pos = np.arange(len(labels))\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "#     sections = min(len(values[0]), sections)\n",
    "    sections = min(len(label_data[C]), sections)\n",
    "    for n in range(sections):\n",
    "        sections_ = min(len(values[0]), sections)\n",
    "        barwidth = (0.5/sections_)\n",
    "#         V = [x[n] if n <= len(x) else None for x in values]\n",
    "        V = []\n",
    "        for x in values:\n",
    "            try:\n",
    "                V.append(x[n])\n",
    "            except:\n",
    "                V.append(0)\n",
    "#         print(C, n, min(len(label_data[C]), n))\n",
    "        section_labels = label_data[C][min(len(label_data[C])-1, n)]\n",
    "        section_pos = pos+(barwidth*n)\n",
    "#         print(V, section_labels, section_pos, labels)\n",
    "        ax.bar(section_pos, V, barwidth, label=section_labels)\n",
    "\n",
    "#     ticks = pos-(barwidth*sections/4)\n",
    "    ticks = pos+(barwidth*round((sections-2)/2))\n",
    "    # print(ticks)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(labels)\n",
    "\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_xlabel('Contributing Models')\n",
    "    \n",
    "    s = l[reduce] if reduced else ''\n",
    "    title = f'{combiner_names[C]} Combiner | {s} Accuracy by Model'\n",
    "    \n",
    "#         title += ' - '+l[reduce]\n",
    "    ax.set_title(title, pad=30)\n",
    "#     ax.margins(0.5)\n",
    "#     plt.subplots_adjust(left=0.3)\n",
    "\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n",
    "#     fig.tight_layout(pad=1.5)\n",
    "    return ax\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816dc57c-915e-4682-9051-16193c454b05",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "Visualizes each model/combiner's loss values on the training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348d3931-c548-4346-8bcd-d7a01119120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the combiner indices and generate the graph for each one\n",
    "for i in range(2):\n",
    "    grouped_plot(C=i, sections=4, reduce=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f49e2a2-9f65-413b-a20a-06419f2f078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a CSV file and return a (nested) list of its rows (split on commas)\n",
    "# Params:\n",
    "#     filename: the name of the CSV file\n",
    "#     *or* [a and b]; the model number (e.g., \"2\") and dataset (e.g., \"5ktrain\")\n",
    "def read_csv(filename=None, a=None, b=None, s=1):\n",
    "    if not filename:\n",
    "        filename = f'./model_{int(a)}_{b}.csv'\n",
    "#     If file extension is missing, assume CSV\n",
    "    if not filename.endswith('.csv'):\n",
    "        filename += '.csv'\n",
    "    with open(filename) as predictions:\n",
    "        reader = csv.reader(predictions)\n",
    "        data = list(reader)\n",
    "        data.sort(key=lambda d: d[s])\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed815938-2396-4666-806b-a3020b9b6ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data = []\n",
    "for f in [5]:\n",
    "    imdb_data.extend(read_csv(f'imdb_train_{f}k.csv', s=2))\n",
    "imdb_data.sort(key=lambda d: d[2])\n",
    "print([d[2] for d in imdb_data[:3]])\n",
    "imdb_data = [d[1:] for d in imdb_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a2440-3ae4-4f5e-8603-38b0242cf40a",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff1870-677f-45d8-bdf9-deb36c584c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = []\n",
    "# for f in range(2):\n",
    "dataset = '5ktrain'\n",
    "plot_size = (12, 6)\n",
    "fig = plt.figure(figsize=plot_size)\n",
    "plt.rcParams[\"figure.figsize\"] = plot_size\n",
    "fig, ax = plt.subplots()\n",
    "# ax.set_xscale('log')\n",
    "# ax.set_yscale('log')\n",
    "\n",
    "# Load predictions for each model\n",
    "for f in list('1234'):\n",
    "    p.append(read_csv(a=f, b=dataset))\n",
    "\n",
    "# p.append(read_csv('imdb_train_5k'))\n",
    "p.append(imdb_data)\n",
    "# p.append(read_csv(a='2', b=dataset))\n",
    "\n",
    "# Convert strings in data to floating-point numbers\n",
    "def prep_data(n):\n",
    "    return [float(m[0]) for m in n if '.' in m[0]]\n",
    "\n",
    "# Prepare the data to graph\n",
    "points = [prep_data(n) for n in p]\n",
    "# ax.scatter(*points)\n",
    "\n",
    "# Draw the plot\n",
    "ax.scatter(*points[:2], alpha=0.5, s=5, c=points[4], cmap='inferno')\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd5ed45-447e-4852-91e3-42bdec356ff6",
   "metadata": {},
   "source": [
    "### Losses\n",
    "\n",
    "Plot the distribution of loss values (absolute value of difference between predicted and actual value) for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02aa2ee-0db7-4e57-93aa-e7b65890136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [0, 1, 2, 3]\n",
    "A = 1 / len(target)\n",
    "# losses = np.abs(np.array(points[1]) - np.array(points[2]))\n",
    "losses = []\n",
    "for t in target:\n",
    "    ys = np.array(prep_data(imdb_data))\n",
    "#     ys = np.random.randint(0, 2, ys.shape)\n",
    "    loss = np.abs(np.array(prep_data(p[t])) - np.array(ys))\n",
    "    losses.append(loss)\n",
    "x = plt.hist(losses, bins=15, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea71cc-b33b-4a57-9a56-bc5d47193512",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(prep_data(p[0])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fb0b23-3bcd-49e8-a2f3-820d2763c55a",
   "metadata": {},
   "source": [
    "### Prediction Distribution\n",
    "\n",
    "Plot a histogram of the models' predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba20a2-5d4a-4d9c-b560-e4834d48527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [0, 1, 2, 3]\n",
    "A = 1 / len(target)\n",
    "# for t in target:\n",
    "#     plt.hist(points[t], bins=50, alpha=A)\n",
    "\n",
    "plt.style.use('seaborn-deep')\n",
    "x = plt.hist([points[t] for t in target], bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b92c53-5995-4a93-a97f-1f3e24ad66eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p[3][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0bb0e8-991e-4e9c-a72b-3bf66f06eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\n",
    "    'a': 5,\n",
    "    'b': 35,\n",
    "    'c': 2.8\n",
    "}\n",
    "# plt.plot(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
